from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.google.cloud.hooks.gcs import GCSHook
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
from google.cloud import bigquery

from datetime import datetime, timedelta, timezone

import sys
import os
import pytz
import json
import logging

sys.path.append(os.path.join(os.environ["AIRFLOW_HOME"], "dags/crawler"))
from crawler.naver_flight_tester import NaverFlightCrawler

# í•œêµ­ ì‹œê°„ ì„¤ì •
KST = pytz.timezone('Asia/Seoul')

# GCS ë° BigQuery ì„¤ì •
GCS_BUCKET_NAME = "ticket_checker_bucket"
GCS_OBJECT_NAME = "naver_flight_data/flight_results.json"

GCP_CONN_ID = "google_cloud_default"
BQ_PROJECT_ID = "ticketchecker-449405"
BQ_DATASET_NAME = "ticket_checker"
BQ_TABLE_NAME = "flight_lowest_price"

# âœ… ê¸°ì¡´ GCS ì—°ê²° í™•ì¸ ë°©ì‹ ìœ ì§€
def check_gcs_connection():
    hook = GCSHook()
    try:
        # âœ… ì—°ê²° í™•ì¸ì„ ìœ„í•´ ì‹¤ì œ ì—…ë¡œë“œ ì‹œë„
        hook.upload(
            bucket_name=GCS_BUCKET_NAME, 
            object_name="airflow_check/result.json", 
            data=json.dumps({"result": "success"})
        )
        logging.info("âœ… GCS ì—°ê²° ë° ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ.")
    except Exception as e:
        hook.upload(
            bucket_name=GCS_BUCKET_NAME, 
            object_name="airflow_check/result.json", 
            data=json.dumps({"result": str(e)})
        )
        logging.error(f"ğŸ”´ GCS ì—°ê²° ì‹¤íŒ¨: {e}")

# âœ… XComì— ë°ì´í„° ì €ì¥ ì¶”ê°€ (ì¶”ê°€ëœ ë¶€ë¶„)
def fetch_flight_data_and_upload(**kwargs):
    departure_airport = "ICN"
    arrival_airport = "KIX"

    crawler = NaverFlightCrawler()
    
    start_date = datetime.now(KST) + timedelta(days=10)
    end_date = start_date + timedelta(days=150)

    flight_data_list = []
    current_date = start_date
    while current_date <= end_date:
        departure_date = current_date.strftime("%Y%m%d")
        flight_data = crawler.fetch_flight_data(departure_airport, arrival_airport, departure_date)
        if flight_data:
            flight_data_list.append({"date": departure_date, "data": flight_data})
        current_date += timedelta(days=1)

    if flight_data_list:
        hook = GCSHook()
        hook.upload(
            bucket_name=GCS_BUCKET_NAME,
            object_name=GCS_OBJECT_NAME,
            data=json.dumps(flight_data_list, ensure_ascii=False, indent=4)
        )
        logging.info(f"âœ… ë°ì´í„° GCS ì—…ë¡œë“œ ì™„ë£Œ: {GCS_OBJECT_NAME}")

# âœ… XCom ëŒ€ì‹  GCSì— ì €ì¥í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •
def fetch_transform_data(**kwargs):
    gcs_hook = GCSHook(gcp_conn_id=GCP_CONN_ID)
    raw_data = gcs_hook.download(GCS_BUCKET_NAME, GCS_OBJECT_NAME)
    data = json.loads(raw_data) if raw_data else []

    transformed_data = []
    for entry in data:
        date = entry.get("date")
        fares = entry.get("data", {}).get("fares", {})

        min_fare = float('inf')
        for flight_id, fare_info in fares.items():
            fare_types = fare_info.get("fare", {}).get("A01", [])
            for fare_option in fare_types:
                adult_fare = fare_option.get("Adult", {}).get("Fare")
                if adult_fare is not None:
                    try:
                        adult_fare = int(adult_fare)
                        min_fare = min(min_fare, adult_fare)
                    except ValueError:
                        logging.warning(f"âš ï¸ ìš”ê¸ˆ ë³€í™˜ ì‹¤íŒ¨: {adult_fare}")

        transformed_data.append({"date": date, "lowest_fare": min_fare if min_fare != float('inf') else None})

    logging.info(f"ğŸŸ¢ ë³€í™˜ëœ ë°ì´í„°: {transformed_data}")

    # âœ… ë³€í™˜ëœ ë°ì´í„°ë¥¼ GCSì— ì €ì¥ (XCom ëŒ€ì‹ )
    transformed_gcs_path = f"naver_flight_data/transformed_flight_results.json"
    gcs_hook.upload(
        bucket_name=GCS_BUCKET_NAME,
        object_name=transformed_gcs_path,
        data=json.dumps(transformed_data, ensure_ascii=False, indent=4)
    )

    # âœ… XComì—ëŠ” ë³€í™˜ ë°ì´í„°ê°€ ì €ì¥ëœ GCS ê²½ë¡œë§Œ ì €ì¥
    kwargs['ti'].xcom_push(key='transformed_data_gcs_path', value=transformed_gcs_path)


# âœ… BigQuery ì—…ë¡œë“œ ì‹œ XCom ì‚¬ìš© ë°©ì‹ ìˆ˜ì •
def upload_to_bigquery(**kwargs):
    bigquery_hook = BigQueryHook(gcp_conn_id=GCP_CONN_ID, use_legacy_sql=False)
    gcs_hook = GCSHook(gcp_conn_id=GCP_CONN_ID)

    # âœ… XComì—ì„œ ë³€í™˜ ë°ì´í„°ê°€ ì €ì¥ëœ GCS ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    transformed_gcs_path = kwargs['ti'].xcom_pull(task_ids='fetch_transform_data', key='transformed_data_gcs_path')
    
    if not transformed_gcs_path:
        logging.warning("ğŸ”´ No transformed data GCS path found in XCom.")
        return

    # âœ… ë³€í™˜ ë°ì´í„°ë¥¼ GCSì—ì„œ ë‹¤ìš´ë¡œë“œ
    raw_transformed_data = gcs_hook.download(GCS_BUCKET_NAME, transformed_gcs_path)
    transformed_data = json.loads(raw_transformed_data) if raw_transformed_data else []

    if not transformed_data:
        logging.warning("ğŸ”´ No data to insert into BigQuery.")
        return

    client = bigquery_hook.get_client()
    table_id = f"{BQ_PROJECT_ID}.{BQ_DATASET_NAME}.{BQ_TABLE_NAME}"
    temp_table_id = f"{BQ_PROJECT_ID}.{BQ_DATASET_NAME}.temp_{BQ_TABLE_NAME}"

    # âœ… BigQuery í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìƒì„±
    try:
        client.get_table(table_id)
    except Exception:
        schema = [
            bigquery.SchemaField("date", "STRING"),
            bigquery.SchemaField("lowest_fare", "INTEGER"),
        ]
        client.create_table(bigquery.Table(table_id, schema=schema))

    job_config = bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE")
    client.load_table_from_json(transformed_data, temp_table_id, job_config=job_config).result()

    merge_query = f"""
    MERGE `{table_id}` AS target
    USING `{temp_table_id}` AS source
    ON target.date = source.date
    WHEN MATCHED THEN
        UPDATE SET target.lowest_fare = source.lowest_fare
    WHEN NOT MATCHED THEN
        INSERT (date, lowest_fare) VALUES (source.date, source.lowest_fare)
    """
    client.query(merge_query).result()
    client.delete_table(temp_table_id, not_found_ok=True)

    logging.info("âœ… BigQuery ë°ì´í„° ì—…ë¡œë“œ ë° MERGE ì™„ë£Œ.")

# âœ… DAG ì„¤ì • ë³€ê²½
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 3, 1, tzinfo=KST),  # âœ… start_dateë¥¼ ê³ ì •ëœ ê³¼ê±° ë‚ ì§œë¡œ ì„¤ì • (ìˆ˜ì •ë¨)
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    dag_id='naver_flight_pipeline',
    default_args=default_args,
    schedule_interval='0 */12 * * *',
    catchup=False,
) as dag:
    gcs_task = PythonOperator(
        task_id='check_gcs',
        python_callable=check_gcs_connection
    )

    crawl_and_upload_task = PythonOperator(
        task_id="fetch_and_upload_flight_data",
        python_callable=fetch_flight_data_and_upload
    )

    # âœ… provide_context ì œê±° (Airflow 2.xì—ì„œëŠ” ë¶ˆí•„ìš”)
    fetch_transform_task = PythonOperator(
        task_id="fetch_transform_data",
        python_callable=fetch_transform_data
    )

    upload_task = PythonOperator(
        task_id="upload_to_bigquery",
        python_callable=upload_to_bigquery
    )

    gcs_task >> crawl_and_upload_task >> fetch_transform_task >> upload_task