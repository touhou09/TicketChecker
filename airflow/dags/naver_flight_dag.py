import sys
import os

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.google.cloud.hooks.gcs import GCSHook
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook

from datetime import datetime, timedelta

import pytz
import json
import logging

sys.path.append(os.path.join(os.environ["AIRFLOW_HOME"], "dags/crawler"))
from crawler.naver_flight_tester import NaverFlightCrawler

# í•œêµ­ ì‹œê°„ ì„¤ì •
KST = pytz.timezone('Asia/Seoul')

# GCS ë° BigQuery ì„¤ì •
GCS_BUCKET_NAME = "ticket_checker_bucket"
GCS_OBJECT_NAME = "naver_flight_data/flight_results.json"

GCP_CONN_ID = "google_cloud_default"
BQ_PROJECT_ID = "ticketchecker-449405"
BQ_DATASET_NAME = "ticket_checker"
BQ_TABLE_NAME = "flight_lowest_price"

# GCS ì—°ê²° í™•ì¸ í•¨ìˆ˜
def check_gcs_connection():
    hook = GCSHook()
    try:
        hook.upload(bucket_name=GCS_BUCKET_NAME, object_name="airflow_check/result.json", data=json.dumps({"result": "success"}))
    except Exception as e:
        hook.upload(bucket_name=GCS_BUCKET_NAME, object_name="airflow_check/result.json", data=json.dumps({"result": str(e)}))

# í•­ê³µí¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ JSONìœ¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def fetch_flight_data_and_upload():
    departure_airport = "ICN"  # ì¸ì²œ
    arrival_airport = "KIX"    # ì˜¤ì‚¬ì¹´ ê°„ì‚¬ì´

    crawler = NaverFlightCrawler()
    
    # ì˜¤ëŠ˜ ë‚ ì§œë¶€í„° 5ê°œì›”(150ì¼) ë™ì•ˆì˜ ë°ì´í„° ìˆ˜ì§‘
    start_date = datetime.now(KST) + timedelta(days=1)
    end_date = start_date + timedelta(days=150)

    flight_data_list = []
    current_date = start_date
    while current_date <= end_date:
        departure_date = current_date.strftime("%Y%m%d")
        print(f"[INFO] {departure_date} ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

        flight_data = crawler.fetch_flight_data(departure_airport, arrival_airport, departure_date)
        if flight_data:
            flight_data_list.append({
                "date": departure_date,
                "data": flight_data
            })

        current_date += timedelta(days=1)

    if flight_data_list:
        today = start_date.strftime("%Y%m%d")
        gcs_path = GCS_OBJECT_NAME.format(date=today)
        
        # JSON ë°ì´í„°ë¥¼ GCSì— ì—…ë¡œë“œ
        hook = GCSHook()
        hook.upload(bucket_name=GCS_BUCKET_NAME, object_name=gcs_path, data=json.dumps(flight_data_list, ensure_ascii=False, indent=4))
        print(f"[INFO] {gcs_path} ê²½ë¡œë¡œ ì—…ë¡œë“œ ì™„ë£Œ.")
    else:
        print("[INFO] ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def fetch_transform_data():
    gcs_hook = GCSHook(gcp_conn_id=GCP_CONN_ID)
    
    # âœ… GCSì—ì„œ JSON ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    raw_data = gcs_hook.download(GCS_BUCKET_NAME, GCS_OBJECT_NAME)
    data = json.loads(raw_data)  # JSON íŒŒì‹±

    transformed_data = []

    for entry in data:  
        date = entry.get("date")  
        fares = entry.get("data", {}).get("fares", {})

        # âœ… í•´ë‹¹ ë‚ ì§œì˜ ìµœì†Œ ìš”ê¸ˆì„ ì°¾ê¸° ìœ„í•œ ì´ˆê¸°ê°’ ì„¤ì •
        min_fare = float('inf')

        for flight_id, fare_info in fares.items():
            # âœ… A01 ìš”ê¸ˆ íƒ€ì…ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            fare_types = fare_info.get("fare", {}).get("A01", [])

            for fare_option in fare_types:
                adult_fare = fare_option.get("Adult", {}).get("Fare")
                if adult_fare is not None:
                    try:
                        adult_fare = int(adult_fare)  # âœ… ë¬¸ìì—´ì¸ ê²½ìš° ì •ìˆ˜ ë³€í™˜
                        if adult_fare < min_fare:
                            min_fare = adult_fare
                    except ValueError:
                        logging.warning(f"âš ï¸ ìš”ê¸ˆ ë³€í™˜ ì‹¤íŒ¨: {adult_fare}")

        # âœ… í•´ë‹¹ ë‚ ì§œì— ìµœì†Œ ìš”ê¸ˆì´ ì¡´ì¬í•˜ë©´ ì €ì¥, ì—†ìœ¼ë©´ None
        transformed_data.append({
            "date": date,
            "lowest_fare": min_fare if min_fare != float('inf') else None
        })

    logging.info(f"ğŸŸ¢ ë³€í™˜ëœ ë°ì´í„° (ê° ë‚ ì§œë³„ ìµœì†Œ ìš”ê¸ˆ): {transformed_data}")
    return transformed_data

def upload_to_bigquery(**kwargs):
    """
    ë³€í™˜ëœ ë°ì´í„°ë¥¼ BigQueryì— ì—…ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    """
    bigquery_hook = BigQueryHook(gcp_conn_id=GCP_CONN_ID, use_legacy_sql=False)
    
    # ë³€í™˜ëœ ë°ì´í„°ë¥¼ XComì—ì„œ ê°€ì ¸ì˜¤ê¸°
    transformed_data = kwargs['ti'].xcom_pull(task_ids='fetch_transform_data')

    if not transformed_data:
        print("ğŸ”´ No data to insert into BigQuery. ë³€í™˜ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return

    print(f"ğŸŸ¢ BigQueryì— ì—…ë¡œë“œí•  ë°ì´í„°: {transformed_data}")  # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€

    # BigQuery Client ê°€ì ¸ì˜¤ê¸°
    client = bigquery_hook.get_client()

    table_id = f"{BQ_PROJECT_ID}.{BQ_DATASET_NAME}.{BQ_TABLE_NAME}"

    # âœ… insert_all() ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì‚½ì…
    errors = client.insert_rows_json(
        table=table_id,
        json_rows=transformed_data  # âœ… JSON í˜•íƒœ ê·¸ëŒ€ë¡œ ì „ë‹¬
    )

    if errors:
        print(f"âŒ BigQuery ì—…ë¡œë“œ ì‹¤íŒ¨: {errors}")
        raise RuntimeError(f"BigQuery ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {errors}")

    print("âœ… BigQuery ì—…ë¡œë“œ ì™„ë£Œ!")

# DAG ì •ì˜
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime.now(KST),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    dag_id='naver_flight_pipeline',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False,
) as dag:
    gcs_task = PythonOperator(
        task_id='check_gcs',
        python_callable=check_gcs_connection
    )

    crawl_and_upload_task = PythonOperator(
        task_id="fetch_and_upload_flight_data",
        python_callable=fetch_flight_data_and_upload
    )

    # GCSì—ì„œ JSON ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ë³€í™˜í•˜ëŠ” Task
    fetch_transform_task = PythonOperator(
        task_id="fetch_transform_data",
        python_callable=fetch_transform_data,
        provide_context=True,
        dag=dag,
    )

    # ë³€í™˜ëœ ë°ì´í„°ë¥¼ BigQueryì— ì—…ë¡œë“œí•˜ëŠ” Task
    upload_task = PythonOperator(
        task_id="upload_to_bigquery",
        python_callable=upload_to_bigquery,
        provide_context=True,
        dag=dag,
    )

    gcs_task >> crawl_and_upload_task >> fetch_transform_task >> upload_task
